#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥–æ–≤ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ OpenRouter
–£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
"""
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich import box

console = Console()


def load_cache_logs(log_file_path: str) -> List[Dict]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–≥–∏ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ —Ñ–∞–π–ª–∞"""
    try:
        with open(log_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—é
        entries = content.split('---\n')
        logs = []
        
        for entry in entries:
            entry = entry.strip()
            if entry:
                try:
                    log_data = json.loads(entry)
                    logs.append(log_data)
                except json.JSONDecodeError:
                    continue
        
        return logs
    except Exception as e:
        console.print(f"[red]–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–≥–æ–≤: {e}[/]")
        return []


def analyze_cache_usage(logs: List[Dict]) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–µ—à–∞ –∏–∑ –ª–æ–≥–æ–≤"""
    analysis = {
        "total_requests": len(logs),
        "models_tested": set(),
        "tests_executed": set(),
        "cache_stats": {
            "cache_creation_tokens": 0,
            "cache_read_tokens": 0,
            "total_tokens": 0,
            "requests_with_cache_info": 0
        },
        "cache_efficiency": {},
        "token_breakdown": []
    }
    
    for log in logs:
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        model = log.get("model", "unknown")
        test_name = log.get("test_name", "unknown")
        usage = log.get("usage", {})
        cache_info = log.get("cache_info", {})
        
        analysis["models_tested"].add(model)
        analysis["tests_executed"].add(test_name)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã
        total_tokens = usage.get("total_tokens", 0)
        cache_creation = usage.get("cache_creation_input_tokens", 0)
        cache_read = usage.get("cache_read_input_tokens", 0)
        
        analysis["cache_stats"]["total_tokens"] += total_tokens
        analysis["cache_stats"]["cache_creation_tokens"] += cache_creation
        analysis["cache_stats"]["cache_read_tokens"] += cache_read
        
        if cache_info:
            analysis["cache_stats"]["requests_with_cache_info"] += 1
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ –ø–æ —Ç–æ–∫–µ–Ω–∞–º
        token_entry = {
            "timestamp": log.get("timestamp"),
            "model": model,
            "test": test_name,
            "attempt": log.get("attempt", 1),
            "total_tokens": total_tokens,
            "prompt_tokens": usage.get("prompt_tokens", 0),
            "completion_tokens": usage.get("completion_tokens", 0),
            "cache_creation": cache_creation,
            "cache_read": cache_read,
            "has_cache_info": bool(cache_info),
            "cache_fields": list(cache_info.keys()) if cache_info else []
        }
        
        analysis["token_breakdown"].append(token_entry)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º sets –≤ lists –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
    analysis["models_tested"] = list(analysis["models_tested"])
    analysis["tests_executed"] = list(analysis["tests_executed"])
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
    cache_stats = analysis["cache_stats"]
    if cache_stats["cache_creation_tokens"] > 0:
        analysis["cache_efficiency"]["efficiency_ratio"] = (
            cache_stats["cache_read_tokens"] / cache_stats["cache_creation_tokens"]
        )
    else:
        analysis["cache_efficiency"]["efficiency_ratio"] = 0
    
    analysis["cache_efficiency"]["cache_hit_percentage"] = (
        cache_stats["requests_with_cache_info"] / analysis["total_requests"] * 100
        if analysis["total_requests"] > 0 else 0
    )
    
    return analysis


def display_analysis(analysis: Dict):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
    console.print(Panel.fit(
        f"[bold cyan]üìä –ê–Ω–∞–ª–∏–∑ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ OpenRouter[/]\n\n"
        f"[green]–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤:[/] {analysis['total_requests']}\n"
        f"[blue]–ú–æ–¥–µ–ª–µ–π –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ:[/] {len(analysis['models_tested'])}\n"
        f"[yellow]–¢–µ—Å—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ:[/] {len(analysis['tests_executed'])}",
        title="üîç –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞",
        border_style="cyan"
    ))
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
    cache_stats = analysis["cache_stats"]
    
    stats_table = Table(title="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è", box=box.ROUNDED)
    stats_table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
    stats_table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="green", justify="right")
    stats_table.add_column("–ü—Ä–æ—Ü–µ–Ω—Ç", style="yellow", justify="right")
    
    total_tokens = cache_stats["total_tokens"]
    cache_creation = cache_stats["cache_creation_tokens"]
    cache_read = cache_stats["cache_read_tokens"]
    
    stats_table.add_row(
        "–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤",
        f"{total_tokens:,}",
        "100.0%"
    )
    
    stats_table.add_row(
        "–¢–æ–∫–µ–Ω—ã —Å–æ–∑–¥–∞–Ω–∏—è –∫–µ—à–∞",
        f"{cache_creation:,}",
        f"{(cache_creation/total_tokens*100):.2f}%" if total_tokens > 0 else "0.00%"
    )
    
    stats_table.add_row(
        "–¢–æ–∫–µ–Ω—ã —á—Ç–µ–Ω–∏—è –∫–µ—à–∞",
        f"{cache_read:,}",
        f"{(cache_read/total_tokens*100):.2f}%" if total_tokens > 0 else "0.00%"
    )
    
    stats_table.add_row(
        "–ó–∞–ø—Ä–æ—Å—ã —Å –∫–µ—à-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π",
        f"{cache_stats['requests_with_cache_info']}",
        f"{analysis['cache_efficiency']['cache_hit_percentage']:.1f}%"
    )
    
    console.print(stats_table)
    
    # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
    efficiency_table = Table(title="–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è", box=box.ROUNDED)
    efficiency_table.add_column("–ú–µ—Ç—Ä–∏–∫–∞", style="cyan")
    efficiency_table.add_column("–ó–Ω–∞—á–µ–Ω–∏–µ", style="green")
    
    efficiency = analysis["cache_efficiency"]
    
    efficiency_table.add_row(
        "–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
        f"{efficiency['efficiency_ratio']:.2f}"
    )
    
    efficiency_table.add_row(
        "–ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ø–∞–¥–∞–Ω–∏–π –≤ –∫–µ—à",
        f"{efficiency['cache_hit_percentage']:.1f}%"
    )
    
    if cache_creation > 0 and cache_read > 0:
        savings = cache_read - cache_creation
        efficiency_table.add_row(
            "–≠–∫–æ–Ω–æ–º–∏—è —Ç–æ–∫–µ–Ω–æ–≤",
            f"{savings:,} —Ç–æ–∫–µ–Ω–æ–≤" if savings > 0 else f"{abs(savings):,} —Ç–æ–∫–µ–Ω–æ–≤ (–ø–µ—Ä–µ—Ä–∞—Å—Ö–æ–¥)"
        )
    
    console.print(efficiency_table)
    
    # –†–∞–∑–±–∏–≤–∫–∞ –ø–æ –º–æ–¥–µ–ª—è–º
    model_breakdown = {}
    for entry in analysis["token_breakdown"]:
        model = entry["model"]
        if model not in model_breakdown:
            model_breakdown[model] = {
                "requests": 0,
                "total_tokens": 0,
                "cache_creation": 0,
                "cache_read": 0,
                "cache_info_requests": 0
            }
        
        stats = model_breakdown[model]
        stats["requests"] += 1
        stats["total_tokens"] += entry["total_tokens"]
        stats["cache_creation"] += entry["cache_creation"]
        stats["cache_read"] += entry["cache_read"]
        if entry["has_cache_info"]:
            stats["cache_info_requests"] += 1
    
    model_table = Table(title="–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–æ–¥–µ–ª—è–º", box=box.ROUNDED)
    model_table.add_column("–ú–æ–¥–µ–ª—å", style="cyan")
    model_table.add_column("–ó–∞–ø—Ä–æ—Å–æ–≤", justify="center")
    model_table.add_column("–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤", justify="right")
    model_table.add_column("–°–æ–∑–¥–∞–Ω–∏–µ –∫–µ—à–∞", justify="right")
    model_table.add_column("–ß—Ç–µ–Ω–∏–µ –∫–µ—à–∞", justify="right")
    model_table.add_column("% —Å –∫–µ—à-–∏–Ω—Ñ–æ", justify="right")
    
    for model, stats in model_breakdown.items():
        cache_info_pct = (stats["cache_info_requests"] / stats["requests"] * 100) if stats["requests"] > 0 else 0
        
        model_table.add_row(
            model,
            str(stats["requests"]),
            f"{stats['total_tokens']:,}",
            f"{stats['cache_creation']:,}",
            f"{stats['cache_read']:,}",
            f"{cache_info_pct:.1f}%"
        )
    
    console.print(model_table)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    if len(sys.argv) < 2:
        console.print("[red]–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python analyze_cache.py <–ø—É—Ç—å_–∫_–ª–æ–≥—É_–∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è>[/]")
        console.print("[yellow]–ü—Ä–∏–º–µ—Ä: python analyze_cache.py logs/prompt_cache_20240830_120000.log[/]")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –ª–æ–≥ —Ñ–∞–π–ª—ã
        logs_dir = Path("logs")
        if logs_dir.exists():
            cache_logs = list(logs_dir.glob("prompt_cache_*.log"))
            if cache_logs:
                console.print(f"\n[blue]–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã –ª–æ–≥–æ–≤ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è:[/]")
                for log_file in sorted(cache_logs):
                    console.print(f"  {log_file}")
        return
    
    log_file_path = sys.argv[1]
    
    if not Path(log_file_path).exists():
        console.print(f"[red]–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {log_file_path}[/]")
        return
    
    console.print(f"[green]–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–≥–æ–≤ –∏–∑:[/] {log_file_path}")
    
    logs = load_cache_logs(log_file_path)
    
    if not logs:
        console.print("[red]–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–≥–∏ –∏–ª–∏ —Ñ–∞–π–ª –ø—É—Å—Ç–æ–π[/]")
        return
    
    console.print(f"[blue]–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π:[/] {len(logs)}")
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–∏
    analysis = analyze_cache_usage(logs)
    
    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    display_analysis(analysis)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    analysis_file = Path("logs") / f"cache_analysis_{timestamp}.json"
    
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    
    console.print(f"\n[green]üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤:[/] {analysis_file}")


if __name__ == "__main__":
    main()
