import argparse
import base64
import json
import os
import sys
import time
import traceback
import re
import openai
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import httpx
from dotenv import load_dotenv
from openai import OpenAI
from rich.console import Console
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.table import Table
from rich import box

console = Console()


class TestResult:
    def __init__(self, status: str, data: Any = None, error: Optional[str] = None):
        self.status = status
        self.data = data
        self.error = error
        self.timestamp = datetime.now()
    
    def to_dict(self) -> Dict:
        return {
            "status": self.status,
            "data": self.data,
            "error": self.error,
            "timestamp": self.timestamp.isoformat()
        }


class OpenRouterTester:
    def __init__(self, api_key: str, base_url: str):
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.results = {}
        self.max_retries = int(os.getenv("MAX_RETRIES", "3"))
        self.retry_delay = int(os.getenv("RETRY_DELAY", "2"))
        self.timeout = int(os.getenv("REQUEST_TIMEOUT", "120"))
        env_mm = os.getenv("MULTIMODAL_MODELS", "").strip()
        if env_mm:
            self.multimodal_models = [m.strip().lower() for m in env_mm.split(',') if m.strip()]
        else:
            self.multimodal_models = [
                "gpt-5",
                "gpt-4o", "gpt-4-vision", "claude-3", "gemini",
                "llama-3.2-90b-vision", "qwen", "glm-4"
            ]
        
        os.makedirs("out", exist_ok=True)
        os.makedirs("logs", exist_ok=True)
        
        self.log_file = f"logs/test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    def log(self, module: str, method: str, level: str, message: str, error: Optional[Exception] = None):
        timestamp = datetime.now().isoformat()
        log_entry = f"[{timestamp}] [{level}] [{module}::{method}] {message}"
        
        if error:
            log_entry += f"\nError: {str(error)}\n{traceback.format_exc()}"
        
        color_map = {
            "DEBUG": "dim",
            "INFO": "cyan", 
            "WARN": "yellow",
            "ERROR": "red"
        }
        
        console.print(f"[{color_map.get(level, 'white')}]{log_entry}[/]")
        
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(log_entry + "\n")
    
    def retry_call(self, func, module: str, method: str, description: str):
        for attempt in range(1, self.max_retries + 1):
            try:
                self.log(module, method, "DEBUG", f"–ü–æ–ø—ã—Ç–∫–∞ {attempt}/{self.max_retries}: {description}")
                return func()
            except Exception as e:
                if attempt == self.max_retries:
                    self.log(module, method, "ERROR", f"–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã –¥–ª—è {description}", e)
                    raise
                self.log(module, method, "WARN", f"–û—à–∏–±–∫–∞ –Ω–∞ –ø–æ–ø—ã—Ç–∫–µ {attempt}: {e}")
                time.sleep(self.retry_delay * attempt)
    
    def test_chat(self, model: str) -> TestResult:
        module, method = "Chat", "test_basic"
        
        try:
            self.log(module, method, "INFO", f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞—Ç–∞ –¥–ª—è {model}")
            
            messages = [
                {"role": "system", "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É."},
                {"role": "user", "content": "–ù–∞–∑–æ–≤–∏ —Å—Ç–æ–ª–∏—Ü—É –§—Ä–∞–Ω—Ü–∏–∏ –∏ –≥–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω–∏—è –ü–∞—Ä–∏–∂–∞. –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º."}
            ]
            
            response = self.retry_call(
                lambda: self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=100,
                    temperature=0.7
                ),
                module, method, "chat completion"
            )
            
            content = response.choices[0].message.content
            usage = response.usage
            
            self.log(module, method, "INFO", 
                f"–£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç: {content[:100]}... | –¢–æ–∫–µ–Ω—ã: {usage.total_tokens}")
            
            return TestResult("success", {
                "response": content,
                "usage": usage.model_dump() if hasattr(usage, 'model_dump') else None
            })
            
        except Exception as e:
            self.log(module, method, "ERROR", f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞ –¥–ª—è {model}", e)
            return TestResult("failed", error=str(e))
    
    def test_stream(self, model: str) -> TestResult:
        module, method = "Stream", "test_sse"
        
        try:
            self.log(module, method, "INFO", f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –¥–ª—è {model}")
            
            messages = [
                {"role": "user", "content": "–ù–∞–ø–∏—à–∏ —Ö–∞–π–∫—É –ø—Ä–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ Go."}
            ]
            
            stream = self.retry_call(
                lambda: self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    stream=True,
                    max_tokens=150
                ),
                module, method, "stream creation"
            )
            
            chunks = []
            chunk_count = 0
            
            for chunk in stream:
                chunk_count += 1
                if chunk.choices[0].delta.content:
                    chunks.append(chunk.choices[0].delta.content)
            
            result = "".join(chunks)
            self.log(module, method, "INFO", 
                f"–°—Ç—Ä–∏–º –∑–∞–≤–µ—Ä—à–µ–Ω: {chunk_count} —á–∞–Ω–∫–æ–≤, {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            return TestResult("success", {
                "response": result,
                "chunks": chunk_count
            })
            
        except Exception as e:
            self.log(module, method, "ERROR", f"–û—à–∏–±–∫–∞ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –¥–ª—è {model}", e)
            return TestResult("failed", error=str(e))
    
    def test_vision(self, model: str, image_path: str) -> TestResult:
        module, method = "Vision", "test_image"
        
        try:
            if not Path(image_path).exists():
                self.log(module, method, "WARN", f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {image_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                return TestResult("skipped", error="Image not found")
            
            lower_model = model.lower()
            is_multimodal = any(mm in lower_model for mm in self.multimodal_models)
            
            if not is_multimodal:
                self.log(module, method, "INFO", f"–ü—Ä–æ–ø—É—Å–∫ vision –¥–ª—è {model} - –Ω–µ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è")
                return TestResult("skipped", error="Not a multimodal model")
            
            self.log(module, method, "INFO", f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ vision –¥–ª—è {model}")
            
            with open(image_path, "rb") as f:
                image_data = base64.b64encode(f.read()).decode()
            
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "–û–ø–∏—à–∏ —á—Ç–æ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º: —Ü–≤–µ—Ç–∞, –æ–±—ä–µ–∫—Ç—ã, –∫–æ–º–ø–æ–∑–∏—Ü–∏—è."},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
                        }
                    ]
                }
            ]
            
            response = self.retry_call(
                lambda: self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    max_tokens=200
                ),
                module, method, "vision analysis"
            )
            
            content = response.choices[0].message.content
            self.log(module, method, "INFO", f"Vision –∞–Ω–∞–ª–∏–∑: {content[:150]}...")
            
            return TestResult("success", {"response": content})
            
        except Exception as e:
            self.log(module, method, "ERROR", f"–û—à–∏–±–∫–∞ vision –¥–ª—è {model}", e)
            return TestResult("failed", error=str(e))
    
    def test_json_mode(self, model: str) -> TestResult:
        module, method = "JSON", "test_structured"
        
        try:
            self.log(module, method, "INFO", f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ JSON mode –¥–ª—è {model}")
            
            messages = [
                {
                    "role": "system",
                    "content": "–¢—ã –≤–æ–∑–≤—Ä–∞—â–∞–µ—à—å —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–π JSON. –ù–∏–∫–∞–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤–Ω–µ JSON."
                },
                {
                    "role": "user",
                    "content": "–°–æ–∑–¥–∞–π JSON —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ–≥–æ–¥–µ: city (—Å—Ç—Ä–æ–∫–∞), temperature_c (—á–∏—Å–ª–æ), humidity (—á–∏—Å–ª–æ 0-100), conditions (—Å—Ç—Ä–æ–∫–∞). –ì–æ—Ä–æ–¥ - –ú–æ—Å–∫–≤–∞."
                }
            ]
            
            response = self.retry_call(
                lambda: self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    response_format={"type": "json_object"},
                    max_tokens=150
                ),
                module, method, "json generation"
            )
            
            content = response.choices[0].message.content
            
            try:
                parsed = json.loads(content)
                self.log(module, method, "INFO", f"JSON –≤–∞–ª–∏–¥–Ω—ã–π: {json.dumps(parsed, ensure_ascii=False)}")
                return TestResult("success", {"response": parsed})
            except json.JSONDecodeError as e:
                self.log(module, method, "WARN", f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON: {content[:100]}")
                return TestResult("failed", {"response": content}, error=str(e))
                
        except Exception as e:
            self.log(module, method, "ERROR", f"–û—à–∏–±–∫–∞ JSON mode –¥–ª—è {model}", e)
            return TestResult("failed", error=str(e))
    
    def test_harmony_format(self, model: str) -> TestResult:
        module, method = "Harmony", "test_structured"
        
        try:
            if "gpt-oss" not in model.lower():
                self.log(module, method, "INFO", f"–ü—Ä–æ–ø—É—Å–∫ Harmony –¥–ª—è {model} - —Ç–æ–ª—å–∫–æ –¥–ª—è gpt-oss –º–æ–¥–µ–ª–µ–π")
                return TestResult("skipped", error="Not a gpt-oss model")
            
            self.log(module, method, "INFO", f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Harmony format –¥–ª—è {model}")
            
            schema = {
                "type": "object",
                "properties": {
                    "reasoning": {
                        "type": "object",
                        "properties": {
                            "problem": {"type": "string"},
                            "approach": {"type": "string"},
                            "steps": {
                                "type": "array",
                                "items": {"type": "string"}
                            },
                            "conclusion": {"type": "string"}
                        },
                        "required": ["problem", "approach", "steps", "conclusion"]
                    }
                },
                "required": ["reasoning"]
            }
            
            messages = [
                {
                    "role": "system",
                    "content": "Reasoning: high\n–¢—ã —Ä–µ—à–∞–µ—à—å –∑–∞–¥–∞—á–∏ –ø–æ—à–∞–≥–æ–≤–æ. –í–æ–∑–≤—Ä–∞—â–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON."
                },
                {
                    "role": "user",
                    "content": "–†–µ—à–∏ –∑–∞–¥–∞—á—É: –£ –ú–∞—à–∏ –±—ã–ª–æ 15 —è–±–ª–æ–∫. –û–Ω–∞ –æ—Ç–¥–∞–ª–∞ 7 —è–±–ª–æ–∫ –ü–µ—Ç–µ –∏ 3 —è–±–ª–æ–∫–∞ –ö–∞—Ç–µ. –°–∫–æ–ª—å–∫–æ —è–±–ª–æ–∫ –æ—Å—Ç–∞–ª–æ—Å—å —É –ú–∞—à–∏?"
                }
            ]
            
            response = self.retry_call(
                lambda: self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    response_format={
                        "type": "json_schema",
                        "json_schema": {
                            "name": "reasoning_response",
                            "schema": schema,
                            "strict": True
                        }
                    },
                    max_tokens=500
                ),
                module, method, "harmony structured output"
            )

            content = response.choices[0].message.content if response.choices and response.choices[0].message else None

            if not content or not content.strip():
                self.log(module, method, "WARN", "–ü—É—Å—Ç–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç –º–æ–¥–µ–ª–∏ (empty content) ‚Äî –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON")
                return TestResult("failed", {"response": content}, error="Empty response content")

            def try_parse(text: str) -> Tuple[Optional[dict], Optional[str]]:
                """–ü–æ–ø—ã—Ç–∞—Ç—å—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏."""
                try:
                    return json.loads(text), None
                except json.JSONDecodeError as err:
                    match = re.search(r'\{[\s\S]*\}', text)
                    if match:
                        candidate = match.group(0)
                        try:
                            return json.loads(candidate), None
                        except json.JSONDecodeError as err_inner:
                            return None, f"Primary parse error: {err}; Extracted segment parse error: {err_inner}"
                    return None, f"Primary parse error: {err}; no JSON object found"

            parsed, parse_error = try_parse(content)
            if parsed is None:
                self.log(module, method, "WARN", f"–ù–µ–≤–∞–ª–∏–¥–Ω—ã–π Harmony JSON. –û—à–∏–±–∫–∞: {parse_error}. –§—Ä–∞–≥–º–µ–Ω—Ç: {content[:160]}")
                return TestResult("failed", {"response": content}, error=parse_error)

            reasoning = parsed.get("reasoning") if isinstance(parsed, dict) else None
            missing = []
            if isinstance(reasoning, dict):
                for key in ["problem", "approach", "steps", "conclusion"]:
                    if key not in reasoning:
                        missing.append(key)
            else:
                missing.append("reasoning")

            if missing:
                self.log(module, method, "WARN", f"JSON –ø–æ–ª—É—á–µ–Ω, –Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–æ–ª—è: {', '.join(missing)}")
                return TestResult("failed", {"response": parsed}, error=f"Missing fields: {', '.join(missing)}")

            steps_count = len(reasoning.get("steps", [])) if isinstance(reasoning.get("steps"), list) else 0
            self.log(module, method, "INFO", f"Harmony –æ—Ç–≤–µ—Ç –≤–∞–ª–∏–¥–µ–Ω: {steps_count} —à–∞–≥–æ–≤")
            return TestResult("success", {"response": parsed})

        except Exception as e:
            self.log(module, method, "ERROR", f"–û—à–∏–±–∫–∞ Harmony –¥–ª—è {model}", e)
            return TestResult("failed", error=str(e))
    
    def test_tool_calling(self, model: str) -> TestResult:
        module, method = "Tools", "test_functions"
        
        try:
            lowered = model.lower()

            whitelist_raw = os.getenv("TOOL_MODELS", "").strip()
            if whitelist_raw:
                whitelist = [w.strip().lower() for w in whitelist_raw.split(',') if w.strip()]
                if not any(w in lowered for w in whitelist):
                    self.log(module, method, "INFO", f"–ü—Ä–æ–ø—É—Å–∫ tool calling –¥–ª—è {model} (–Ω–µ –≤ TOOL_MODELS)")
                    return TestResult("skipped", error="Model not in TOOL_MODELS whitelist")
            else:
                heuristic_keys = ["gpt", "claude", "llama", "qwen", "deepseek", "mistral", "command", "sonar", "o3", "reasoning"]
                if not any(k in lowered for k in heuristic_keys):
                    self.log(module, method, "INFO", f"–ü—Ä–æ–ø—É—Å–∫ tool calling –¥–ª—è {model} (–Ω–µ –ø—Ä–æ—Ö–æ–¥–∏—Ç —ç–≤—Ä–∏—Å—Ç–∏–∫—É)")
                    return TestResult("skipped", error="Heuristic skip: likely no tool support")

            self.log(module, method, "INFO", f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ tool calling –¥–ª—è {model}")
            
            tools = [
                {
                    "type": "function",
                    "function": {
                        "name": "get_weather",
                        "description": "–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â—É—é –ø–æ–≥–æ–¥—É –≤ –≥–æ—Ä–æ–¥–µ",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "location": {
                                    "type": "string",
                                    "description": "–ì–æ—Ä–æ–¥, –Ω–∞–ø—Ä–∏–º–µ—Ä '–ú–æ—Å–∫–≤–∞'"
                                },
                                "units": {
                                    "type": "string",
                                    "enum": ["celsius", "fahrenheit"],
                                    "description": "–ï–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è"
                                }
                            },
                            "required": ["location"]
                        }
                    }
                },
                {
                    "type": "function",
                    "function": {
                        "name": "calculate",
                        "description": "–í—ã–ø–æ–ª–Ω–∏—Ç—å –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "expression": {
                                    "type": "string",
                                    "description": "–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è"
                                }
                            },
                            "required": ["expression"]
                        }
                    }
                }
            ]
            
            messages = [
                {"role": "user", "content": "–ö–∞–∫–∞—è –ø–æ–≥–æ–¥–∞ –≤ –ü–∞—Ä–∏–∂–µ? –ò —Å–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 25 * 4?"}
            ]
            
            try:
                response = self.retry_call(
                    lambda: self.client.chat.completions.create(
                        model=model,
                        messages=messages,
                        tools=tools,
                        tool_choice="auto",
                        max_tokens=200
                    ),
                    module, method, "tool calling"
                )
            except Exception as call_err:
                err_text = str(call_err)
                status_code = getattr(call_err, 'status_code', None)
                if (status_code == 404) or '404' in err_text or 'NotFound' in err_text:
                    self.log(module, method, "WARN", f"–ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç tool calling (404). –ü—Ä–æ–ø—É—Å–∫.")
                    return TestResult("skipped", error="Tool calling unsupported (404)")
                raise
            
            tool_calls = response.choices[0].message.tool_calls
            
            if tool_calls:
                results = []
                for call in tool_calls:
                    func_name = call.function.name
                    func_args = json.loads(call.function.arguments)
                    results.append({
                        "tool": func_name,
                        "arguments": func_args,
                        "id": call.id
                    })
                    self.log(module, method, "INFO", f"Tool call: {func_name}({func_args})")
                
                return TestResult("success", {"tool_calls": results})
            else:
                self.log(module, method, "WARN", "–ú–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ tool calls")
                return TestResult("failed", error="No tool calls returned")
                
        except Exception as e:
            self.log(module, method, "ERROR", f"–û—à–∏–±–∫–∞ tool calling –¥–ª—è {model}", e)
            return TestResult("failed", error=str(e))

    def test_image_generation(self, model: str, prompt: Optional[str] = None) -> TestResult:
        """–¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –º–æ–¥–µ–ª–µ–π, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏—Ö image generation API.

        –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        ('image', 'dall', 'gpt-image', 'flux', 'sd', 'stable') —á—Ç–æ–±—ã –Ω–µ —Å–ª–∞—Ç—å –Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –∑–∞–ø—Ä–æ—Å—ã.
        """
        module, method = "ImageGen", "test_image_generation"
        try:
            lowered = model.lower()
            if not any(k in lowered for k in ["image", "dall", "gpt-image", "flux", "sd", "stable"]):
                self.log(module, method, "INFO", f"–ü—Ä–æ–ø—É—Å–∫ image generation –¥–ª—è {model} - –≤–µ—Ä–æ—è—Ç–Ω–æ –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
                return TestResult("skipped", error="Model not recognized as image generator")

            prompt = prompt or "A minimalist flat illustration of a friendly cyberpunk cat coding at a holographic laptop, vibrant neon palette"
            self.log(module, method, "INFO", f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ image generation –¥–ª—è {model}")

            start = time.time()
            image_b64 = None
            generation_mode = "images.generate"
            try:
                resp = self.client.images.generate(model=model, prompt=prompt, size="512x512")
                image_b64 = resp.data[0].b64_json if resp and resp.data else None
            except Exception as direct_err:
                self.log(module, method, "WARN", f"–ü—Ä—è–º–æ–π images API –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª ({direct_err}); fallback —á–µ—Ä–µ–∑ chat –ø–æ–ø—ã—Ç–∫–æ–π –ø–æ–ª—É—á–∏—Ç—å data URI")
                generation_mode = "chat-fallback"
                try:
                    chat_resp = self.client.chat.completions.create(
                        model=model,
                        messages=[{"role": "user", "content": f"Generate an image for this prompt and return ONLY a JSON object: {{\"image_base64\": <base64 PNG>}}. Prompt: {prompt}"}],
                        max_tokens=1200,
                        temperature=0.7
                    )
                    content = chat_resp.choices[0].message.content
                    try:
                        data = json.loads(content)
                        image_b64 = data.get("image_base64") if isinstance(data, dict) else None
                    except Exception:
                        match = re.search(r"data:image/(?:png|jpeg);base64,([A-Za-z0-9+/=]+)", content or "")
                        if match:
                            image_b64 = match.group(1)
                except Exception as fallback_err:
                    return TestResult("failed", error=f"Both image API and chat fallback failed: {fallback_err}")

            if not image_b64:
                return TestResult("failed", error="No base64 image data returned")

            out_dir = Path("out")
            out_dir.mkdir(exist_ok=True)
            file_path = out_dir / f"generated_{int(time.time())}.png"
            try:
                with open(file_path, "wb") as f:
                    f.write(base64.b64decode(image_b64))
            except Exception as write_err:
                return TestResult("failed", error=f"Failed to write image file: {write_err}")

            elapsed = time.time() - start
            self.log(module, method, "INFO", f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ ({generation_mode}) –∑–∞ {elapsed:.2f}—Å -> {file_path}")
            return TestResult("success", {"file": str(file_path), "mode": generation_mode, "elapsed_seconds": elapsed})
        except Exception as e:
            self.log(module, method, "ERROR", f"–û—à–∏–±–∫–∞ image generation –¥–ª—è {model}", e)
            return TestResult("failed", error=str(e))
    
    def test_batch(self, model: str, prompts: Optional[List[str]] = None) -> TestResult:
        module, method = "Batch", "test_multiple"
        
        try:
            if not prompts:
                prompts = [
                    "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ä–µ–∫—É—Ä—Å–∏—è –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏?",
                    "–û–±—ä—è—Å–Ω–∏ –ø—Ä–∏–Ω—Ü–∏–ø DRY –æ–¥–Ω–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º",
                    "–ß—Ç–æ —Ç–∞–∫–æ–µ SOLID –ø—Ä–∏–Ω—Ü–∏–ø—ã?"
                ]
            
            self.log(module, method, "INFO", f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ batch –¥–ª—è {model} —Å {len(prompts)} –∑–∞–ø—Ä–æ—Å–∞–º–∏")
            
            results = []
            start_time = time.time()
            
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=console
            ) as progress:
                task = progress.add_task(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(prompts)} –∑–∞–ø—Ä–æ—Å–æ–≤...", total=len(prompts))
                
                for i, prompt in enumerate(prompts):
                    try:
                        response = self.client.chat.completions.create(
                            model=model,
                            messages=[{"role": "user", "content": prompt}],
                            max_tokens=100
                        )
                        content = response.choices[0].message.content
                        results.append({
                            "prompt": prompt[:50],
                            "response": content[:100],
                            "status": "success",
                            "tokens": response.usage.total_tokens
                        })
                        self.log(module, method, "DEBUG", f"Batch {i+1}/{len(prompts)}: OK")
                    except Exception as e:
                        results.append({
                            "prompt": prompt[:50],
                            "response": None,
                            "status": "failed",
                            "error": str(e)[:100]
                        })
                        self.log(module, method, "WARN", f"Batch {i+1}/{len(prompts)}: FAIL - {e}")
                    
                    progress.advance(task)
            
            elapsed = time.time() - start_time
            success_count = sum(1 for r in results if r["status"] == "success")
            
            self.log(module, method, "INFO", 
                f"Batch –∑–∞–≤–µ—Ä—à–µ–Ω: {success_count}/{len(prompts)} —É—Å–ø–µ—à–Ω–æ –∑–∞ {elapsed:.2f}—Å")
            
            return TestResult("success" if success_count > 0 else "failed", {
                "total": len(prompts),
                "success": success_count,
                "failed": len(prompts) - success_count,
                "elapsed_seconds": elapsed,
                "results": results
            })
            
        except Exception as e:
            self.log(module, method, "ERROR", f"–û—à–∏–±–∫–∞ batch –¥–ª—è {model}", e)
            return TestResult("failed", error=str(e))
    
    def run_full_test_suite(self, models: List[str], categories: List[str]):
        module, method = "Suite", "run_all"
        
        self.log(module, method, "INFO", f"–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(models)} –º–æ–¥–µ–ª–µ–π, {len(categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
        
        vision_image = os.getenv("VISION_IMAGE", "payloads/cat.jpeg")
        
        table = Table(title="üöÄ OpenRouter API Test Results", box=box.ROUNDED)
        table.add_column("–ú–æ–¥–µ–ª—å", style="cyan", width=40)
        
        for category in categories:
            table.add_column(category.title(), justify="center", width=12)
        
        for model in models:
            self.results[model] = {}
            row = [model]
            
            console.rule(f"[bold cyan]–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {model}[/]")
            
            for category in categories:
                if category == "chat":
                    result = self.test_chat(model)
                elif category == "stream":
                    result = self.test_stream(model)
                elif category == "vision":
                    result = self.test_vision(model, vision_image)
                elif category == "json":
                    result = self.test_json_mode(model)
                elif category == "harmony":
                    result = self.test_harmony_format(model)
                elif category == "tools":
                    result = self.test_tool_calling(model)
                elif category == "batch":
                    result = self.test_batch(model)
                elif category == "imagegen":
                    result = self.test_image_generation(model)
                else:
                    result = TestResult("skipped", error=f"Unknown category: {category}")
                
                self.results[model][category] = result.to_dict()
                
                status_icon = {
                    "success": "‚úÖ",
                    "failed": "‚ùå",
                    "skipped": "‚è≠Ô∏è"
                }.get(result.status, "‚ùì")
                
                row.append(status_icon)
            
            table.add_row(*row)
        
        console.print("\n")
        console.print(table)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"out/test_report_{timestamp}.json"
        
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2, default=str)
        
        self.log(module, method, "INFO", f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
        
        total_tests = len(models) * len(categories)
        success_tests = sum(
            1 for m in self.results.values() 
            for c in m.values() 
            if c["status"] == "success"
        )
        
        console.print(Panel.fit(
            f"[green bold]–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ![/]\n\n"
            f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\n"
            f"‚Ä¢ –ú–æ–¥–µ–ª–µ–π –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ: {len(models)}\n"
            f"‚Ä¢ –ö–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–µ—Å—Ç–æ–≤: {len(categories)}\n"
            f"‚Ä¢ –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total_tests}\n"
            f"‚Ä¢ –£—Å–ø–µ—à–Ω—ã—Ö: {success_tests}\n"
            f"‚Ä¢ –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {(success_tests/total_tests*100):.1f}%\n\n"
            f"üìÅ –§–∞–π–ª—ã:\n"
            f"‚Ä¢ –û—Ç—á–µ—Ç: {report_file}\n"
            f"‚Ä¢ –õ–æ–≥–∏: {self.log_file}",
            title="‚ú® –ò—Ç–æ–≥–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è",
            border_style="green"
        ))


def main():
    load_dotenv()
    
    parser = argparse.ArgumentParser(description="OpenRouter API Test Suite")
    parser.add_argument("--models", nargs="+", help="–ú–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
    parser.add_argument("--categories", nargs="+", help="–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–µ—Å—Ç–æ–≤")
    parser.add_argument("--api-key", help="API –∫–ª—é—á OpenRouter")
    parser.add_argument("--base-url", help="Base URL –¥–ª—è API")
    parser.add_argument("--verbose", action="store_true", help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥")
    
    args = parser.parse_args()
    
    api_key = args.api_key or os.getenv("ROUTER_API_KEY")
    base_url = args.base_url or os.getenv("ROUTER_BASE_URL", "http://localhost:8080/v1")
    
    if not api_key:
        console.print("[red]‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–∫–∞–∑–∞–Ω API –∫–ª—é—á. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ROUTER_API_KEY –≤ .env[/]")
        sys.exit(1)
    
    models = args.models or os.getenv("TEST_MODELS", "").split(",")
    if not models or models == ['']:
        models = ["openai/gpt-4o", "openai/gpt-oss-120b", "google/gemma-3n-e4b-it:free"]
    
    categories = args.categories or os.getenv("TEST_CATEGORIES", "").split(",")
    if not categories or categories == ['']:
        categories = ["chat", "stream", "vision", "json", "harmony", "tools", "batch"]
    
    if args.verbose:
        os.environ["LOG_LEVEL"] = "DEBUG"
    
    console.print(Panel.fit(
        f"[bold cyan]OpenRouter API Test Suite[/]\n\n"
        f"üîó Endpoint: {base_url}\n"
        f"ü§ñ –ú–æ–¥–µ–ª–∏: {', '.join(models)}\n"
        f"üß™ –¢–µ—Å—Ç—ã: {', '.join(categories)}",
        title="‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è",
        border_style="cyan"
    ))
    
    tester = OpenRouterTester(api_key, base_url)
    
    try:
        tester.run_full_test_suite(models, categories)
    except KeyboardInterrupt:
        tester.log("Main", "main", "WARN", "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        console.print("\n[yellow]‚ö†Ô∏è –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ[/]")
        sys.exit(130)
    except Exception as e:
        tester.log("Main", "main", "ERROR", "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞", e)
        console.print(f"\n[red]üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}[/]")
        sys.exit(1)


if __name__ == "__main__":
    main()